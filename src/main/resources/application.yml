spring:
  application:
    name: kafka-integration

  ## 카프카 설정
  ## Offset (컨슈머가 읽은 메시지 위치) 자동 커밋 대신 ack-mode RECORD 를 사용해 메시지 단위로 커밋, 리스너에서 관리함
  kafka:
    bootstrap-servers: localhost:29092  # Kafka 브로커 주소
    
    consumer:
      auto-offset-reset: earliest  # 오프셋 초기 위치 (earliest: 가장 처음부터 메시지 읽음)
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer  # 키 디시리얼라이저
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer  # 값 디시리얼라이저
      enable-auto-commit: false  # 자동 커밋 비활성화 (수동 커밋 사용)
      properties:
        session.timeout.ms: 45000  # 세션 타임아웃 (ms) - 컨슈머가 살아있는 신호 전송 주기
        max.poll.records: 500  # 단일 poll 요청시 가져올 최대 레코드 수, 처리 시간과 배치크기 사이의 균형 조절 가능
        max.poll.interval.ms: 300000  # poll 간 최대 간격 (ms) - 컨슈머가 살아있음을 알림
    
    listener:
      missing-topics-fatal: false  # 존재하지 않는 토픽이어도 오류 무시
      ack-mode: RECORD  # 레코드별 커밋 모드 (각 메시지 처리 후 커밋 enable-auto-commit: false 설정과 연계)
      concurrency: 1  # 리스너별 스레드 실행 수
      poll-timeout: 3000  # 리스너가 호출할 poll 메서드 타임아웃 (ms)
      type: single  # 단일 메시지 처리 모드 (vs. batch)
    
    admin:
      auto-create: true  # kafkaListener 에 정의된 토픽이 없을 경우 자동 생성 여부 설정

    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer  # 키 시리얼라이저
      value-serializer: org.apache.kafka.common.serialization.StringSerializer  # 값 시리얼라이저

# 파일 감시 설정
file:
  watch:
    directory: ${file.watch.dir:./file}  # 감시할 디렉토리 (기본값: ./file)
    pattern: "*.xml"  # 감시할 파일 패턴 (XML 파일만)

# 로깅 레벨 설정
logging:
  level:
    root: INFO
    pompom.kafkaintegration: DEBUG  # 애플리케이션 로깅 레벨 설정
    org.apache.kafka: WARN  # 모든 Kafka 클라이언트 로그 레벨 조정
    org.springframework.kafka: WARN  # 스프링 Kafka 로그 레벨 조정
    # 특정 Kafka 컴포넌트만 디버그하려면 아래 주석 해제
    # org.springframework.kafka.listener: DEBUG